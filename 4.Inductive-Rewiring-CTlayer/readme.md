# Inductive-Rewiring-CTLayer

## âœï¸ TL;DR

---

<aside>
ğŸ’¡ A layer that learns the commute times and uses them as a relevant function for edge re-weighting performs preliminary studies on the use of CT-LAYER for homophilic and heterophilic node classification tasks

Commute Timeì„ í•™ìŠµí•˜ê³  Edge re-weightingì„ í•˜ê¸° ìœ„í•œ í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ëŠ” ë ˆì´ì–´

â€» **Commute Time**: The Commute Time between two nodes is defined as the sum of the expected times it takes for the random walker to travel from one node to the other and then return.

</aside>

## âœï¸ Introduction

---

- **Background**
    
    : ê¸°ì¡´ Massage Passing Neural Network (MPNN) ê·¸ë˜í”„ ê³ ìœ ì˜ ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ 2ê°€ì§€ ì œí•œ ì‚¬í•­ ë°œìƒí•©ë‹ˆë‹¤.
    
    1. ë³µì¡í•œ ê·¸ë˜í”„ êµ¬ì¡°ì—ì„œ ìµœìƒì˜ ê²°ê³¼ëŠ”, **ì ì€ ìˆ˜ì˜ ë ˆì´ì–´ì—ì„œë§Œ ë°œìƒ**í•œë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.
        
        â›” ë ˆì´ì–´ê°€ ë§ì€ ë„¤íŠ¸ì›Œí¬ëŠ” Over-smoothing ë° Over-Squahsing ë¬¸ì œ ë°œìƒ
        
        - **Over-smoothing:** ê·¸ë˜í”„ ì‹ ê²½ë§ layer ìˆ˜ê°€ ì¦ê°€í•  ìˆ˜ë¡, ì •ì ì˜ ì„ë² ë”©ì´ ì„œë¡œ ìœ ì‚¬í•´ì§€ëŠ” í˜„ìƒ
        - **Over squashing:** ë§ì€ nodesê°€ bottleneck nodeë¥¼ ì§€ë‚˜ë©´, messagesê°€ ì§€ë‚˜ì¹˜ê²Œ squashing/compressionì´ ë˜ì–´ messagesê°€ ì†ìƒë˜ëŠ” í˜„ìƒ
    2. MPNNì˜ ê¹Šì´ê°€ ê·¸ë˜í”„ì˜ ì§ê²½ë³´ë‹¤ ì‘ì„ ë•Œ, **Under-reaching í˜„ìƒ**ì´ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        - **Under-reaching:** MPNNì´ ê·¸ë˜í”„ì˜ ì „ì²´ êµ¬ì¡°ì— ì˜ì¡´í•˜ëŠ” ì •ë³´ë¥¼ ìº¡ì²˜í•˜ì§€ ëª»í•  ë¿ë”ëŸ¬, ë©€ë¦¬ ìˆëŠ” ë…¸ë“œê°„ì— ì •ë³´ê°€ ì œëŒ€ë¡œ ë„ë‹¬í•˜ì§€ëŠ” ëª»í•˜ëŠ” í˜„ìƒ
    
    : ìœ„ì˜ ë‘ê°œ ë¬¸ì œë¥¼ í•´ê²° í•˜ê¸° ìœ„í•´, CT-Layer ì™€ Gap-Layerê°€ ê³ ì•ˆ ë˜ì—ˆìŠµë‹ˆë‹¤.
    : ë³¸ê²©ì ìœ¼ë¡œ CT-Layerì— ì„¤ëª…í•˜ê¸°ì— ì•ì„œ, The LovÃ¡sz Boundì— ëŒ€í•˜ì—¬ ì•Œê³  ìˆì–´ì•¼í•©ë‹ˆë‹¤.
    

- **The LovÃ¡sz Bound**
    
    : LovÃ¡sz BoundëŠ” í†µê·¼ ì‹œê°„(ìœ íš¨ ì €í•­ ê±°ë¦¬)ê³¼ ë„¤íŠ¸ì›Œí¬ì˜ ìŠ¤í™íŠ¸ëŸ¼ ê°­ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•œ ë¶€ë“±ì‹ì…ë‹ˆë‹¤ (Arnaiz-Rodriguez et al. â€˜22). 
    : ì—¬ê¸°ì„œ í†µê·¼ ì‹œê°„(CT)ì€ í™•ì‚° ì´ë¡ , ìŠ¤í™íŠ¸ëŸ¼ ê°­ì€ ê³¡ë¥  ì´ë¡ ê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì´ ë‘˜ì€ ëª¨ë‘ ë¯¸ë¶„ ê¸°í•˜í•™ì— ê·¼ê±°í•˜ê³  ìˆìŠµë‹ˆë‹¤.
    : LovÃ¡sz Bound ë¶€ë“±ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
    
    $$
    \left| \frac{CT_{uv}}{vol(G)}- \left( \frac{1}{d_u}+\frac{1}{d_v} \right) \right| \leq \frac{1}{\lambda_2^\prime} \frac{1}{d_{min}}
    $$
    
    : ìœ„ì˜ ì‹ì—ì„œ $CT_{uv}$ëŠ” ë‘ ì •ì  uì™€ v ì‚¬ì´ì˜ í†µê·¼ ì‹œê°„(CT)ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ì‹ì€ í†µê·¼ ì‹œê°„ì´ ì–´ë–»ê²Œ ë„¤íŠ¸ì›Œí¬ì˜ ìŠ¤í™íŠ¸ëŸ¼ ê°­ê³¼ ê´€ë ¨ ë˜ëŠ”ì§€ ì„¤ëª…í•´ ì¤ë‹ˆë‹¤. 
    : ìˆ˜ì‹ì˜ ì™¼ìª½ ë¶€ë¶„ì€ í†µê·¼ ì‹œê°„ì„ ì „ì²´ ê·¸ë˜í”„ì˜ ë¶€í”¼ë¡œ ë‚˜ëˆˆ ê°’ê³¼ $\left( \frac{1}{d_u}+\frac{1}{d_v} \right)$ì˜ ì°¨ì´ì˜ ì ˆëŒ“ê°’ì´ë©°, ì‹¤ì œ í†µê·¼ ì‹œê°„ê³¼ ê·¸ë˜í”„ ì´ë¡ ì—ì„œ ê¸°ëŒ€ë˜ëŠ” í†µê·¼ ì‹œê°„ ê°„ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
    : ìˆ˜ì‹ì˜ ì˜¤ë¥¸ìª½ ë¶€ë¶„ì€ ë‘ ë²ˆì§¸ ê°€ì¥ ì‘ì€ ê³ ìœ ê°’ì˜ ì—­ìˆ˜ì¸ $\lambda_2^\prime$ (Fiedler value ë¼ê³ ë„ ë¶ˆë¦°ë‹¤)ì™€ ìµœì†Œ ì°¨ìˆ˜($d_{min}$)ì˜ ì—­ìˆ˜ì˜ ê³±ìœ¼ë¡œ ë‚˜ëˆˆ ê°’ì´ë©°, ìŠ¤í™íŠ¸ëŸ¼ ê°­ê³¼ ìµœì†Œ ì°¨ìˆ˜ì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤. 
    : $\lambda^{\prime}_2$ì˜ ê°’ì€ ê·¸ë˜í”„ê°€ ì–¼ë§ˆë‚˜ ì˜ ì—°ê²°ë˜ì–´ ìˆëŠ”ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤.
    : ë”°ë¼ì„œ, ìœ„ì˜ ì‹ì€ í†µê·¼ ì‹œê°„ê³¼ ìŠ¤í™íŠ¸ëŸ¼ ê°­ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì œí•œí•˜ê³ , ë‘ ì •ì  ì‚¬ì´ì˜ í†µê·¼ ì‹œê°„ì´ ìŠ¤í™íŠ¸ëŸ¼ ê°­ì— ì–´ë–»ê²Œ ì˜í–¥ì„ ë°›ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì´ë¥¼ í†µí•´ í†µê·¼ ì‹œê°„ê³¼ ê·¸ë˜í”„ì˜ ê¸°í•˜í•™ì  íŠ¹ì„± ì‚¬ì´ì˜ ì—°ê²°ì„±ì„ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    

- **Commute Time (CT, í†µê·¼ì‹œê°„)**
    
    : ê·¸ë˜í”„ ì´ë¡ ì—ì„œ í†µê·¼ ì‹œê°„(*Commute Time; CT*) ì€ ë‘ ê°œì˜ ì •ì ì‚¬ì´ì˜ ìµœë‹¨ ê²½ë¡œ ê¸¸ì´ê°€ ì•„ë‹Œ **ë‘ê°œ ì •ì ì‚¬ì´ì˜ í†µê³¼í•˜ëŠ” ê¸°ëŒ€ê°’**ìœ¼ë¡œ ì •ì˜ ë©ë‹ˆë‹¤. 
    : í†µê·¼ ì‹œê°„ì€ ë³´í†µ ëœë¤ ì›Œí¬(*Random Walk*) ê°œë…ì„ ì‚¬ìš©í•˜ì—¬ ì •ì˜ë˜ë©°, ëœë¤ ì›Œì»¤ê°€ ë…¸ë“œ $i$ì—ì„œ ë…¸ë“œ $j$ë¡œ ì´ë™í•˜ê³ , ë‹¤ì‹œ ë…¸ë“œ $i$ë¡œ ëŒì•„ì˜¤ê¸°ê¹Œì§€ ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ëˆ„ì í•˜ì—¬ ì¸¡ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    
    <aside>
    ğŸ’¡ $CT_{uv}(=H_{uv} + H_{vu})$
    âœ… ë‘ ë…¸ë“œ($u, v$) ì‚¬ì´ì—,  $u$ì—ì„œ $v$ë¥¼ ì¹˜ê³ (*hit*) ë‹¤ì‹œ $u$ë¡œ ëŒì•„ì˜¤ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„
    
    </aside>
    
    <aside>
    ğŸ’¡ **ëœë¤ ì›Œí¬ (Random Walk)**
    âœ… ë¬´ì‘ìœ„ë¡œ ì •ì ê°„ì„ ì´ë™í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ë¥¼ ë§í•˜ë©°, ê° ì •ì ì—ì„œ ë‹¤ë¥¸ ì •ì ìœ¼ë¡œ ì´ë™í•  í™•ë¥ ì´ ì •ì ê°„ì˜ ì—°ê²°ì„±ì— ì˜í•´ ê²°ì •ë©ë‹ˆë‹¤.
    
    </aside>
    

## âœï¸ CT-Layer (Theory)

---

- **Commute Time Embedding (CTE)**
    
    : CT-Layerì—ì„œ ì €ìë“¤ì€ í†µê·¼ ì‹œê°„(CT)ì„ ì§ì ‘ ê³„ì‚°í•œ ê²ƒì´ ì•„ë‹ˆë¼ ì„ë°°ë”©(CTE)ì„ ë„ì…í•˜ì—¬ CTë¥¼ ê·¼ì‚¬ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í•˜ì˜€ìŠµë‹ˆë‹¤. 
    : ë”°ë¼ì„œ, CT-Layerë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„  CTEì— ëŒ€í•œ ì´í•´ê°€ ë¨¼ì € í•„ìš”í•©ë‹ˆë‹¤.
    : CTEëŠ” ê° ë…¸ë“œë“¤ì˜ í†µê·¼ì‹œê°„ ì •ë³´ì™€ ìœ í´ë¦¬ë“œ ê³µê°„ì—ì„œì˜ ê±°ë¦¬ ì •ë³´ë¥¼ë°˜ì˜í•œ ì„ë² ë”© ë²¡í„°ì´ë©°, CTEí–‰ë ¬ $**\mathbf{Z}**$ëŠ” ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ ê°€ëŠ¥í•©ë‹ˆë‹¤:
    
    $$
    \mathbf{Z}\coloneqq \sqrt{vol(G)}\Lambda^{-1/2}\mathbf{F}^T
    $$
    
    $$
    CT_{uv} = \|z_u - z_v\|^2
    $$
    
    : CTEë¥¼ í•™ìŠµì‹œí‚¬ ë•Œ ì´ìš©ë˜ëŠ” ìµœì í™” ë¬¸ì œì˜ ëª©ì í•¨ìˆ˜ëŠ” ì¸ì ‘í•œ ë…¸ë“œ ìŒ ì‚¬ì´ì˜ CT ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
    : ê°™ì€ ë§¥ë½ìœ¼ë¡œ, ëª©ì í•¨ìˆ˜ê°€ ì¸ì ‘í•œ ë…¸ë“œ ì‚¬ì´ì˜ ìœ íš¨ ì €í•­ì„ ìµœì†Œí™”í•˜ê±°ë‚˜, í˜¹ì€ CTE ê°„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”í•œë‹¤ê³  í•´ì„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    

- **CT-Layer**
    
    : CT-LayerëŠ” CTEë¥¼ í•™ìŠµí•˜ë„ë¡ ê³ ì•ˆëœ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ëª¨ë“ˆì…ë‹ˆë‹¤. 
    : ê¸°ì¡´ì˜ ê·¸ë˜í”„ì¬ë°°ì„ (*graph rewiring*) ë°©ì‹ë“¤ê³¼ëŠ” ë‹¤ë¥´ê²Œ CT-LayerëŠ” í”„ë¦¬í”„ë¡œì„¸ì‹± ë°©ë²•ì´ ì•„ë‹Œ end-to-end ë°©ì‹ìœ¼ë¡œ ì „ì²´ ëª¨ë¸ê³¼ ê°™ì´ íŠ¸ë ˆì´ë‹ì„ í•©ë‹ˆë‹¤. 
    : ë˜í•œ CT-LayerëŠ” ì¬ë°°ì„ ê³¼ ê´€ë ¨í•œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¼ì²´ ì‚¬ìš©í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì¥ì ì´ ìˆìŠµë‹ˆë‹¤.
    : CT-LayerëŠ” single-layer MPNNê³¼ ê·¸ì— ë”°ë¥¸ ë¶€ì°¨ì ì¸ ì—°ì‚°ë“¤ë¡œ êµ¬ì„±ë˜ì–´ìˆìŠµë‹ˆë‹¤. 
    
    - **ìœ ë„ê³¼ì • (technical details)**
        
        $X\in\mathbb{R}^{V\times H_X}$ë¥¼ ê·¸ë˜í”„ í”¼ì³ ì…ë ¥ê°’ì´ë¼ í•˜ê³ , $Z\in\mathbb{R}^{V\times H_Z}$ë¥¼ MPNNì˜ ê²°ê³¼ê°’ì´ë¼ê³  í• ë•Œ CT-Layerì˜ ìµœì¢… ì•„ì›ƒí’‹ì€ $T^{CT}$ ëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•´ì§‘ë‹ˆë‹¤.
        
        - $z_u=f(x_u,N(u))$
        - $x_u$ì™€ $z_u$ëŠ” ê°ê° $X$ì™€ $Z$ì˜ í–‰(*row*)ë²¡í„°ë“¤ì´ë©° (*i.e.* $n\in V$), $N(u)$ëŠ” ë…¸ë“œ $u$ì˜ ì´ì›ƒ ë…¸ë“œë“¤ì˜ ì§‘í•©ì…ë‹ˆë‹¤.
        - $\mathbf{T}^{CT}=\frac{cdist(\mathbf{Z})}{vol(G)}\odot A$
        - $cdist(\cdot)$ ëŠ” ê±°ë¦¬ ë§¤íŠ¸ë¦­(*metric*)ìœ¼ë¡œì¨ $cdist(Z)\in\mathbb{R}^{V\times V}$ì˜ ì„ì˜ì˜ ìš”ì†Œ(*element*) $c_{ij}$ëŠ” $\|z_i - z_j\|^2$ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤. $vol(G)$ëŠ” ê·¸ë˜í”„ $G$ì˜ ë³¼ë¥¨ì´ë©° (*i.e.* degreeì˜ ì´í•©), $A$ëŠ” $G$ì˜ ì¸ì ‘í–‰ë ¬(*adjacency matrix*), $\odot$ì€ Hadamard product (*i.e.* element-wise multiplication)ì…ë‹ˆë‹¤.
        - ì¦‰, CT-Layerì˜ ì•„ì›ƒí’‹ $\mathbf{T}^{CT}$ ëŠ” ì¼ì¢…ì˜ ì¬ë°°ì„ ëœ ê·¸ë˜í”„(*rewired graph*)ì˜ ìƒˆ ì¸ì ‘í–‰ë ¬ì¸ ì…ˆì´ë©°, $(\frac{cdist(\mathbf{Z})}{vol(G)},\odot)$ ëŠ” ì´ ì¬ë°°ì„  ê³¼ì •ì˜ relevance functionì´ ë˜ëŠ” ì…ˆì´ë‹¤.
        - CT-LayerëŠ” ë‹¤ìŒì˜ ë¡œìŠ¤í•¨ìˆ˜ë¥¼ ìµœì í™”í•˜ì—¬ íŠ¸ë ˆì´ë‹ í•˜ê²Œ ëœë‹¤.
            - $Loss_{_{CT}} = \frac{Tr[\mathbf{Z}^T\mathbf{LZ}]}{Tr[\mathbf{Z}^T\mathbf{DZ}]} + \|\frac{\mathbf{Z}^T\mathbf{Z}}{\|\mathbf{Z}^T\mathbf{Z}\|_{_F}}-\mathbf{I}\|_{_F}$
            - ì²«ë²ˆì§¸ í…€(*term*)ì€ CTë¥¼ ìµœì†Œí™” í•˜ëŠ” ì—­í• ì„ í•˜ë©° (*i.e.* $\|z_i-z_j\|^2,\;\;\forall (i,j)$), ë‘ë²ˆì§¸ í…€ì€ ì •ê·œí™”(*normalizing*) ë° ì§êµì„±(*orthogonality*)ì— ëŒ€í•œ ì œì•½ì‹(*constraints*) ì—­í• ì„ í•œë‹¤.

## ğŸ§‘â€ğŸ’»Â CT-Layer (Hands-on)

---

```python
def dense_CT_rewiring(x, adj, s):
    """
    Args:
        x (dense): feature matrix: BxNxF
        adj (dense): dense adjacency matrix: BxNxN
        s (dense): CT Embedding: BxNxH (H: size of latent space)
    Returns:
        adj: new adjacency = CTdist/vol(G)
        loss: Cut Loss for CT Embedding (s)
        ortho_loss: Loss regularization orthogonality in CT Embedding (s)
    """
    # Activation
    s = torch.tanh(s) # torch.Size([B,N,H])
		H = s.size(-1)
    
    # CT Rewiring
    ## Calculate CT_dist 
    CT_dist = torch.cdist(s,s) # [B,N,H], [B,N,H]-> [B,N,N]
    ## Calculate degree d_flat and degree matrix d
    d_flat = torch.einsum('ijk->ij', adj) # torch.Size([B,N]) 
    d = _rank3_diag(d_flat)  # d torch.Size([B,N,N])
    ## Calculate Vol (volumes): one per graph 
    vol = _rank3_trace(d) # torch.Size([B]) 
    ## Calculate out_adj as CT_dist/vol(G)
    CT_dist = (CT_dist) / vol.unsqueeze(1).unsqueeze(1)
    ## New adjacency matrix (T^{CT})
    adj = CT_dist*adj
    
    # Cut loss
    ## Calculate Laplacian L = D - A 
    L = d - adj
    ## Calculate Tr[S.T*L*S]
    CT_num = _rank3_trace(torch.matmul(torch.matmul(s.transpose(1, 2), L), s))
    ## Calculate Tr[S.T*D*S]
    CT_den = _rank3_trace(torch.matmul(torch.matmul(s.transpose(1, 2), d), s))
		## Cut loss for CT embedding (the first term)
    CT_loss = torch.mean(CT_num / CT_den)

    # Orthogonality regularization loss
    ss = torch.matmul(s.transpose(1, 2), s)
    i_s = torch.eye(H).type_as(ss)
    ortho_loss = torch.norm(
        ss / torch.norm(ss, dim=(-1, -2), keepdim=True) -
        i_s)
    ortho_loss = torch.mean(ortho_loss) # Mean over batch!
    
    return adj, CT_loss, ortho_loss
```

- **Arguments**
    
    : `x`: ê·¸ë˜í”„ í”¼ì³ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ, BxNxF ì°¨ì›ì„ ê°€ì§‘ë‹ˆë‹¤.
    : `adj`: ì›ë³¸ ê·¸ë˜í”„ì˜ ì¸ì ‘í–‰ë ¬ì´ë©°, BxNxN ì°¨ì›ì„ ê°€ì§‘ë‹ˆë‹¤.
    : `s`: CTEë¥¼ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì´ë©°, BxNxH ì°¨ì›ì„ ê°€ì§‘ë‹ˆë‹¤.
    
    â›” BëŠ” ë°°ì¹˜ ì‚¬ì´ì¦ˆ, Nì€ ê·¸ë˜í”„ì˜ (ìµœëŒ€)ë…¸ë“œ ê°œìˆ˜ì…ë‹ˆë‹¤.
    â›” ìœ„ ì½”ë“œëŠ” ì›í™œí•œ ì´í•´ë¥¼ ìœ„í•´ ì›ë³¸ CT-Layer ì½”ë“œë¥¼ ê°„ì†Œí™”í•œ ë²„ì „ì…ë‹ˆë‹¤.
    
- **Activation**
    
    : ëª¨ë¸ íŠ¸ë ˆì´ë‹ì˜ ì•ˆì •í™”ë¥¼ ìœ„í•´ íƒ„ì  íŠ¸ í•˜ì´í¼ë³¼ë¦­(*tanh*) í™œì„±í•¨ìˆ˜(*activation*)ë¥¼ ì·¨í•´ì£¼ì—ˆìŠµë‹ˆë‹¤.
    
- **CT Rewiring**
    
    : ì¬ë°°ì„ ëœ ê·¸ë˜í”„ì˜ ìƒˆ ì¸ì ‘í–‰ë ¬ $\mathbf{T}^{CT}$ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
    : $\mathbf{T}^{CT}=\frac{cdist(\mathbf{Z})}{vol(G)}\odot A$
    : `_rank3_diag()` ëŠ” 1D ë²¡í„°ë¥¼ ëŒ€ê°í–‰ë ¬ë¡œ ë³€í™˜í•´ì£¼ëŠ” í•¨ìˆ˜ì˜ ë°°ì¹˜ ë²„ì „ì´ë©°, `_rank3_trace()` ëŠ” ë©”íŠ¸ë¦­ìŠ¤ì˜ íŠ¸ë ˆì´ìŠ¤ë¥¼ êµ¬í•˜ëŠ” í•¨ìˆ˜ì˜ ë°°ì¹˜ ë²„ì „ìœ¼ë¡œ, DiffWire íŒ¨í‚¤ì§€ì— ì •ì˜ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
    
- **Cut loss**
    
    : $Loss_{_{CT}}$ì—ì„œ ì²«ë²ˆì§¸ í…€  $*\frac{Tr[\mathbf{Z}^T\mathbf{LZ}]}{Tr[\mathbf{Z}^T\mathbf{DZ}]}*$ì„ ê³„ì‚°í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
    : $\mathbf{L}$ë¥¼ ê³„ì‚°í•  ë•Œ, ì¬ë°°ì„ ëœ ê·¸ë˜í”„ì˜ ì¸ì ‘í–‰ë ¬ì´ ì“°ì´ëŠ” ê²ƒì— ì£¼ì˜í•©ë‹ˆë‹¤.
    : ìˆ˜ì‹ì—ì„œ $\mathbf{Z}$ëŠ” `dense_CT_rewiring()` ë‚´ì—ì„œ `s`ë¡œ í‘œí˜„ë˜ì—ˆìŠµë‹ˆë‹¤.
    
- **Orthogonality regularization loss**
    
    : $Loss_{_{CT}}$ì—ì„œ ë‘ë²ˆì§¸ í…€ $*\|\frac{\mathbf{Z}^T\mathbf{Z}}{\|\mathbf{Z}^T\mathbf{Z}\|_{_F}}-\mathbf{I}\|_{_F}*$ì„ ê³„ì‚°í•˜ëŠ” ì½”ë“œì…ë‹ˆë‹¤.
    : $\mathbf{Z}^T\mathbf{Z}$ëŠ” `ss`ë¡œ $\mathbf{I}$ëŠ” `i_s`ë¡œ ê°ê° í‘œí˜„ë˜ì—ˆìœ¼ë©°, ê³„ì‚°í• ë•Œ ë³€ìˆ˜ë“¤ì˜ ì°¨ì›ì„ ì£¼ì˜í•´ ì£¼ëŠ” ê²ƒ ì™¸ì—ëŠ” íŠ¹ë³„íˆ ë³µì¡í•œ ë¶€ë¶„ì€ ì—†ìŠµë‹ˆë‹¤.
    

## âœï¸ GAP-Layer (Theory)

---

- **Main idea**
    
    : GAP-Layerì˜ í•µì‹¬ ë˜ëŠ” ì•„ì´ë””ì–´ëŠ” Lovasz bound ë¶€ë“±ì‹ì˜ **ìš°ë³€í•­**(*right-hand side*)ì„ ì¡°ì ˆí•˜ì—¬ ë°”ìš´ë“œë¥¼ ì™„í™”ì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤. 
    : ì˜ˆë¥¼ ë“¤ì–´, ì¬ë°°ì„ ì„ í†µí•´ Fiedler value ($\lambda_2$ ë˜ëŠ”  $\lambda_2^{\prime}$)ë¥¼ ì¤„ì´ê²Œ ë˜ë©´, Lovasz bound ì œì•½ì‹ì´ ì™„í™”ë˜ì–´ ì„ì˜ì˜ ë‘ ì—£ì§€ë“¤(*edges*) ê°„ì˜ ìœ íš¨ì €í•­(*effective resistance*) ì°¨ì´ê°€ ì»¤ì§ˆ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.
    
    : ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ ë³´ë©´, Fiedler valueë¥¼ ì¤„ì¸ë‹¤ëŠ” ê²ƒì€ ë³‘ëª© ì—£ì§€(*bottleneck edge*)ë“¤ì„ ë”ìš± ê³¼ì¥í•˜ëŠ” íš¨ê³¼ë¥¼ ì¤€ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 
    : ë˜ëŠ”, ê°™ì€ ë§¥ë½ìœ¼ë¡œ, ë³‘ëª© ì—ì§€ì˜ curvatureë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” íš¨ê³¼ë¥¼ ì¤€ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ëŠ” ë³‘ëª© ì—ì§€ì˜ curvatureë¥¼ ì¤„ì´ëŠ” CT-Layerì˜ ì‘ìš©ê³¼ ìƒë°˜ë˜ëŠ” íš¨ê³¼ì…ë‹ˆë‹¤. 
    
    : ì´ëŸ¬í•œ CT-Layerì™€ GAP-Layerì˜ íš¨ê³¼ ì°¨ì´ëŠ” ì•„ë˜ì˜ ê·¸ë¦¼ì—ë„ ì˜ ë“œëŸ¬ë‚˜ ìˆìŠµë‹ˆë‹¤. 
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/63e98a95-defc-49fe-a5bd-8292aa110d9e/Untitled.png)
    
    : ì¼ë°˜ì ìœ¼ë¡œ inter-clusterê°„ ì—°ê²°ë‹¤ë¦¬(*bridge*)ê°€ ë˜ëŠ” ì—ì§€ë“¤ì´ (*i.e.* ë³‘ëª© ì—ì§€) ì ì„ìˆ˜ë¡, ê·¸ë¦¬ê³  ê·¸ ì—°ê²°ë‹¤ë¦¬ì˜ ìƒëŒ€ì ì¸ ë„“ì´ê°€ ì‘ì„ìˆ˜ë¡ (*i.e.* ì—ì§€ ì›¨ì´íŠ¸ì˜ ìƒëŒ€ì  í¬ê¸°ê°€ ì‘ì„ìˆ˜ë¡; ì—ì§€ curvatureê°€ í´ ìˆ˜ë¡) ë³‘ëª©í˜„ìƒì´ ì‹¬í•˜ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
    : ìœ„ì˜ ê·¸ë¦¼ì„ ë³´ë©´ CT-LayerëŠ” ë³‘ëª© ì—ì§€ë“¤ì˜ curvatureë¥¼ ì¦ê°€í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì¬ë°°ì„ ì„ í•œ ë°˜ë©´ GAP-LayerëŠ” ë³‘ëª© ì—ì§€ë“¤ì˜ curvatureë¥¼ ì¦ê°€ì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ ì¬ë°°ì„ ì„ í•œ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.
    
- **GAP-Layer**
    
    : ë©”ì¸ ì•„ì´ë””ì–´ì—ì„œ ì‚´í´ë³¸ ë°”ì™€ ê°™ì´, GAP-Layer ë¬¸ì œëŠ” $\lambda_2:\tilde{A}\rightarrow\mathbb{R}^{+}$ ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ ë„¤íŠ¸ì›Œí¬ ìœ„ìƒì„ ë³´ì¡´í•˜ëŠ” ì¸ì ‘í–‰ë ¬ $\tilde{A}$ë¥¼ ì°¾ëŠ” ë¬¸ì œë¡œ ê·€ê²°ë˜ê²Œ ë©ë‹ˆë‹¤. 
    : íŠ¹íˆ, DiffWire(Arnaiz-Rodriguez et al. â€˜22)ì˜ ì €ìë“¤ì€ ì´ë¥¼ $F_{fiedler}$ì„ ê²½ì‚¬í•˜ê°•ë²•(*gradient descent*)ìœ¼ë¡œ í‘¸ëŠ” ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í•˜ì˜€ìŠµë‹ˆë‹¤.
    : êµ¬ì²´ì ìœ¼ë¡œ $F_{fiedler}$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë˜ë©°, ì´ ê°’ì„ $\tilde{A}$ì— ê´€í•˜ì—¬ ìµœì í™” ì‹œì¼œ GAP-Layerë¥¼ í•™ìŠµì‹œí‚¤ê²Œ ë©ë‹ˆë‹¤.
    
    $$
    F_{fiedler}=\|\tilde{A}-A\|_{_F} + \alpha(\lambda_2(\tilde{A}))^2
    $$
    
    - **ìœ ë„ ê³¼ì • (technical details)**
        
        $F_{fiedler}$ë¥¼ ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ìµœì í™” í•˜ê¸° ìœ„í•´ì„œëŠ” $\nabla_{\tilde{A}}\lambda_2$ì˜ ê°’ì„ êµ¬í• ìˆ˜ ìˆì•„ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜, $\tilde{A}$ì˜ í•¨ìˆ˜ë¡œ í‘œí˜„ë˜ëŠ” $\lambda_2$, ì¦‰, $\lambda_2(\tilde{A})$ëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì£¼ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ, ì €ìëŠ” $\lambda_2(\tilde{A})$ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ (1) $\tilde{A}$ë¥¼ $A$ì™€ ê°™ì€ $\lambda_2$ì˜ ê³µìœ í•˜ëŠ” ì¸ì ‘í–‰ë ¬ì´ë¼ ê°€ì •í•˜ê³ , (2) spectral clusteringì„ ì´ìš©í•´ ratio cutë¬¸ì œë¥¼ í’€ ìˆ˜ ìˆë‹¤(von Luxburg, '07)ëŠ” ì‚¬ì‹¤ì„ í™œìš©í•˜ì˜€ìŠµë‹ˆë‹¤. 
        
        - $\tilde{A}$ì˜ ê·¸ë˜í”„ê°€ ì—°ê²°ë˜ì–´ìˆë‹¤(*connected*)ê³  ê°€ì •í•˜ë©´, $\tilde{\lambda}_1=0$, $\mathbf{f}_1=\mathbf{1}$, $\tilde{\lambda}_2\neq 0$
        - ë”°ë¼ì„œ, $\tilde{\lambda}_2 = \lambda_2 = Tr[\tilde{U}^T\tilde{L}\tilde{U}]\;\; s.t.\;\; \tilde{U}=[\mathbf{f}_1, \mathbf{f}_2]\in\mathbb{R}^{n\times 2},\;\; \tilde{U}\tilde{U}^T=\mathbf{I}_2$
        - $\nabla_{\tilde{A}}\lambda_2 = diag(\textbf{f}_2\textbf{f}_2^T)\textbf{1}\textbf{1}_2 - \textbf{f}_2\textbf{f}_2^T$  (Kang & Tong. '19)
        - $\nabla_{\tilde{A}}F_{fiedler} = 2(\tilde{A}-A) + \frac{\alpha}{2}(diag(\textbf{f}_2\textbf{f}_2^T)\textbf{1}\textbf{1}_2 - \textbf{f}_2\textbf{f}_2^T)\times\lambda_2$
        
        ì¦‰, $\tilde{A}$ì˜ Fiedler vectorì¸ $\textbf{f}_2$ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤ë©´, $F_{fiedler}$ë¥¼ ìµœì†Œí™”í•˜ëŠ” (ë”°ë¼ì„œ, $\lambda_2(\tilde{A})$ë¥¼ ìµœì†Œí™” í•˜ê²Œ ë˜ëŠ”) $\tilde{A}$ë¥¼ êµ¬í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.
        
        ìœ„ ê°€ì •ë“¤ì— ë”°ë¼ $\textbf{f}_2$ëŠ” ê·¼ì‚¬ì ìœ¼ë¡œ $A$ë¥¼ ì¸ì ‘í–‰ë ¬ë¡œ ê°€ì§€ëŠ” ê·¸ë˜í”„ì˜ ratio cutìœ¼ë¡œ ë‘˜ ìˆ˜ ìˆê²Œë©ë‹ˆë‹¤. ë˜í•œ, ê·¸ë˜í”„ ratio cut ë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìµœì í™” ë¬¸ì œë¡œ ì¬êµ¬ì„± ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ (Gallier. â€˜22).
        
        - ì–´ë–¤ ê·¸ë˜í”„ê°€ Kê°œì˜ ê·¸ë£¹($\{V_1, V_2, \cdots, V_k\}$)ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í• ë•Œ, ë‹¤ìŒê³¼ ê°™ì€ íŒŒí‹°ì…˜ í–‰ë ¬ $S$ë¥¼ ì •ì˜í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤
            - $S_i^j = \begin{cases}
            1 ,& v_i \in V_j\\
            0 ,& v_i \notin V_j
            \end{cases}$
        - ë˜í•œ, ìœ„ì™€ ê°™ì´ ì •ì˜ëœ $S$ì— ëŒ€í•´ì„œ ë‹¤ìŒì´ ì„±ë¦½í•˜ê²Œ ë©ë‹ˆë‹¤.
            - $Cut(V_j, \bar{V_j}) = S^jLS^j$
            - $|V_j|=S^jS^j$
        - ë”°ë¼ì„œ, ratio cutë¬¸ì œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìµœì í™” ë¬¸ì œë¡œ í‘œí˜„ë©ë‹ˆë‹¤.
            - $min_{_S}\sum_{j=1}^k\frac{Cut(V_j, \bar{V_j})}{|V_j|} = \sum_{j=1}^k\frac{S^jLS^j}{S^jS^j}\;\; s.t.\;\; S^iS^j=0\; \; \forall i\neq j$
            - ë˜ëŠ”, $min_{_S}Tr[S^TLS]\;\; s.t.\;\; S^TS=I$
        
        ì´ ê´€ê³„ë“¤ì„ ì´ìš©í•˜ì—¬, GAP-Layerì—ì„œëŠ” ì…ë ¥ë°ì´í„° $X$ë¡œë¶€í„° $S$ë¥¼ ì¶”ë¡ í•´ ë‚´ëŠ” MPNN ë ˆì´ì–´ $f(\cdot)$ë¥¼ ratio cut ë¡œìŠ¤í•¨ìˆ˜ì¸ $L_{cut}$ì„ ìµœì†Œí™” ì‹œí‚¤ëŠ” ë°©ë²•ì„ í†µí•´ í•™ìŠµì‹œí‚µë‹ˆë‹¤.
        
        - $L_{cut}=-\frac{Tr[S^TAS]}{Tr[S^TDS]}+\|\frac{S^TS}{\|S^TS\|_{_F}}-\frac{I}{\sqrt{2}}\|_{_F}$
        - $S_{|V|\times 2} = softmax(f(X))$
        - $L_{cut}$ì˜ ì²«ë²ˆì§¸ í•­ì€ ë‹¤ìŒê³¼ ê°™ì€ ê´€ê³„ë¡œ ë¶€í„° ë‚˜ì™”ìŠµë‹ˆë‹¤.
        - $Tr[S^TLS] = Tr[S^T(D-A)S] = Tr[S^TDS]-Tr[S^TAS]$
        - ìµœì¢…ì ìœ¼ë¡œ GAP Layerì˜ ë¡œìŠ¤ëŠ”, $L_{GAP} = L_{cut} + L_{fiedler}$
        

## ğŸ§‘â€ğŸ’»Â GAP-Layer (Hands-on)

---

```python
def dense_mincut_rewiring(x, adj, s): 
		"""
    Args:
        x (dense): feature matrix: BxNxF
        adj (dense): dense adjacency matrix: BxNxN
        s (dense): GAP embedding: BxNx2
    Returns:
        Ac: new adjacency 
        mincut_loss: cut loss
        ortho_loss: orthogonality regularization
    """
    # Hyper parameters
	  k = 2 # We want bipartition to compute spectral gap
    mu = 0.01 # learning rate for \tilde{A}
    lambdaReg = 2.0 # regularization wegith: 0.5*\alpha
    
    # Batched Laplacian 
    d_flat = torch.einsum('ijk->ij', adj) # torch.Size([B,N]) 
    d = _rank3_diag(d_flat) # d torch.Size([B,N,N]) 
    L = d - adj

		# Fiedler vector approximation
    s = torch.softmax(s, dim=-1)
    fiedlers = approximate_Fiedler(s)

    # Recalculate
    der = derivative_of_lambda2_wrt_adjacency(fiedlers)
    fvalues = fiedler_values(adj, fiedlers)
    
    # Calculate \tilde{A}
    Ac = adj.clone()
    for _ in range(5):
      partialJ = 2*(Ac-adj) + 2*lambdaReg*der*fvalues.unsqueeze(1).unsqueeze(2)
      dJ = partialJ + torch.transpose(partialJ,1,2) - 
					 torch.diag_embed(torch.diagonal(partialJ,dim1=1,dim2=2))
      # Update adjacency
      Ac  = Ac - mu*dJ
      Ac = torch.softmax(Ac, dim=-1)
      Ac = Ac*adj
		
    # MinCUT regularization
		## calculate Tr[S.T*A*S]
		mincut_num = _rank3_trace(
				torch.matmul(torch.matmul(s.transpose(1, 2), adj), s))
		## calculate Tr[S.T*D*S]
    mincut_den = _rank3_trace(
        torch.matmul(torch.matmul(s.transpose(1, 2), d), s)) 
    ## calculate the average cut loss
    mincut_loss = torch.mean(-(mincut_num / mincut_den))
    ## Orthogonality regularization.
    ss = torch.matmul(s.transpose(1, 2), s)
    i_s = torch.eye(k).type_as(ss) 
    ortho_loss = torch.norm(
        ss / torch.norm(ss, dim=(-1, -2), keepdim=True) -
        i_s / torch.norm(i_s), dim=(-1, -2))  
    ortho_loss = torch.mean(ortho_loss) 
    
    return  Ac, mincut_loss, ortho_loss
```

- **Arguments**
    
    : `x`: ê·¸ë˜í”„ í”¼ì³ ë§¤íŠ¸ë¦­ìŠ¤ë¡œ, BxNxF ì°¨ì›ì„ ê°€ì§‘ë‹ˆë‹¤.
    : `adj`: ì›ë³¸ ê·¸ë˜í”„ì˜ ì¸ì ‘í–‰ë ¬ì´ë©°, BxNxN ì°¨ì›ì„ ê°€ì§‘ë‹ˆë‹¤.
    : `s`: GAP embeddingì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ì´ë©°, BxNx2 ì°¨ì›ì„ ê°€ì§‘ë‹ˆë‹¤.
    
    â›” `s`ì˜ ë§ˆì§€ë§‰ ì°¨ì›ì´ 2 ì¸ê²ƒì— ì£¼ì˜í•©ë‹ˆë‹¤. ì´ëŠ” ê·¸ë˜í”„ë¥¼ 2ê°œì˜ ê·¸ë£¹(*i.e.*, bipartite)ìœ¼ë¡œ ë‚˜ëˆ„ê³ ì í•˜ëŠ” ëª©ì ì— ê¸°ì¸í•©ë‹ˆë‹¤.
    â›” ìœ„ ì½”ë“œëŠ” ì›í™œí•œ ì´í•´ë¥¼ ìœ„í•´ ì›ë³¸ GAP-Layer ì½”ë“œë¥¼ ê°„ì†Œí™”í•œ ë²„ì „ì…ë‹ˆë‹¤.
    

- **Hyper parameters**
    
    : `k`: í•­ìƒ 2ë¡œ ê³ ì •ëœ ê°’ìœ¼ë¡œ, bipartitionì„ í•˜ê³ ì í•˜ëŠ” ëª©ì ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    : `mu`: $\tilde{A}$ì˜ ëŸ¬ë‹ë ˆì´íŠ¸(*learning rate*)ì´ë‹¤. ì¦‰, $\tilde{A}\leftarrow\tilde{A} -\text{mu}\cdot\nabla_{\tilde{A}}F_{fiedler}$
    : `lambdaReg`: $F_{fiedler}$ì—ì„œ $\alpha$ì— ë¹„ë¡€í•˜ëŠ” ê°’ì…ë‹ˆë‹¤.
    

- **Fiedler vector approximation**
    
    : `s`ë¡œë¶€í„° $\mathbf{f}_2$ë¥¼ ë„ì¶œí•´ë‚´ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
    : ìš°ì„  `s`ë¥¼ softmaxí•¨ìˆ˜ë¥¼ í†µí•´ í‘œì¤€í™” í•´ì£¼ê²Œ ë©ë‹ˆë‹¤. ì´ ê³¼ì •ì€ `approximate_Fiedler()` ì— í•„ìš”í•œ ê²ƒì€ ì•„ë‹ˆì§€ë§Œ, `s`ëŠ” ì´í›„ ë¡œìŠ¤ ê³„ì‚°ì—ë„ ì“°ì´ê¸° ë•Œë¬¸ì—, ëª¨ë¸ í•™ìŠµì„ ì•ˆì •í™” í•´ì£¼ëŠ” íš¨ê³¼ë¥¼ ì¤ë‹ˆë‹¤.
    : `approximate_Fiedler()` ëŠ” `s`ë¥¼ Fiedler vectorë¡œ ë³€í™˜í•´ì£¼ëŠ” í•¨ìˆ˜ì´ë©° $s_i^j$ë¥¼ `s[b]` $\in\mathbb{R}^{N\times 2}$ì˜ ($i,j$) ì»´í¬ë„ŒíŠ¸ë¼ í•  ë•Œ, ë³€í™˜ ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤:
    
    - $\mathbf{f}_2(i) \in\mathbb{R}^N= \begin{cases}1/\sqrt{N},\;\;\;\;\; s_i^1\geq s_i^2\\ -1/\sqrt{N},\;\; s_i^1<s_i^2\end{cases}$

- **Recalculate**
    
    : `derivative_of_lambda2_wrt_adjacency()`ëŠ” $\nabla_{\tilde{A}}\lambda_2$ë¥¼ Fiedler vector $\mathbf{f}_2$ë¥¼ ì´ìš©í•´ì„œ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤
    
    - $\nabla_{\tilde{A}}\lambda_2 = diag(\textbf{f}_2\textbf{f}_2^T)\textbf{1}\textbf{1}_2 - \textbf{f}_2\textbf{f}_2^T$.
    
    : `fiedler_values()`ëŠ” Fiedler value ($\lambda_2$)ë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ì´ë•Œë„ ë§ˆì°¬ê°€ì§€ë¡œ $\mathbf{f}_2$ë¥¼ ì´ìš©í•´ ê³„ì‚°í•˜ë©°, êµ¬ì²´ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ Dirichlet Energyë¥¼ í™œìš©í•´ì„œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    
    - $\lambda_2 = N\times \|\frac{\mathbf{f}_2^T L \mathbf{f}_2}{\mathbf{f}_2^T L_c \mathbf{f}_2}\|$
    - $L$ì€ $A$ ê·¸ë˜í”„ì˜ Laplacian, $L_c$ëŠ” complete graphì˜ Laplacian.

- **Calculate $\tilde{A}$**
    
    : ì¬ë°°ì„ ëœ ê·¸ë˜í”„ì˜ ì¸ì ‘í–‰ë ¬ $\tilde{A}$ë¥¼ ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ êµ¬í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. 
    
    : ê²½ì‚¬í•˜ê°•ë²•ì˜ ê¸°ë³¸ì ì¸ êµ¬ì¡°ì¸ $\tilde{A}\leftarrow\tilde{A}-\eta\cdot\nabla_{\tilde{A}}L_{GAP}$ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ìœ ë„/ë³€í˜•ì´ ì´ë£¨ì–´ ì§‘ë‹ˆë‹¤.
    
    - $\nabla_{\tilde{A}}L_{GAP}=\nabla_{\tilde{A}}(L_{cut} + L_{Fiedler}) = \nabla_{\tilde{A}}L_{Fiedler}$
    - $\nabla_{\tilde{A}}L_{Fiedler}=2(\tilde{A}-A) + \frac{\alpha}{2}(diag(\textbf{f}_2\textbf{f}_2^T)\textbf{1}\textbf{1}_2 - \textbf{f}_2\textbf{f}_2^T)\times\lambda_2$
    - $\nabla_{\tilde{A}}^*L_{Fiedler}=\nabla_{\tilde{A}}L_{Fiedler}+(\nabla_{\tilde{A}}L_{Fiedler})^T - Diag(L_{Fiedler})$
    - $\tilde{A}^{(temp)}\leftarrow\tilde{A}^{(old)}-\eta\cdot\nabla^*_{\tilde{A}}L_{Fiedler}$
    - $\tilde{A}^{(new)}\leftarrow Softmax(\tilde{A}^{(temp)})\odot A$
    
    : $\nabla_{\tilde{A}}^*L_{Fiedler}$ë¡œ ë³€í™˜í•˜ëŠ” ë¶€ë¶„ì€ $\tilde{A}$ì˜ ëŒ€ì¹­ì„± ë³´ì¥ì„ ìœ„í•œ íœ´ë¦¬ìŠ¤í‹±(*heuristic*)í•œ ë°©ë²•ìœ¼ë¡œ ì´í•´ë©ë‹ˆë‹¤ (ì¦‰, ê·¸ë˜í”„ê°€ undirectedë¼ëŠ” ê°€ì •ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤).
    
    : $\tilde{A}$ëŠ” ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ parameterê°€ ì•„ë‹ˆê¸° ë•Œë¬¸ì— autogradë¥¼ í†µí•´ end-to-endë°©ì‹ìœ¼ë¡œ ìµœì í™” í•˜ê¸°ê°€ í˜ë“­ë‹ˆë‹¤. ì €ìëŠ” ì´ ë¬¸ì œë¥¼ ë‚´ë¶€ë£¨í”„(*inner-iteraton or inner-loop*)ë°©ì‹ìœ¼ë¡œ í•´ê²°í•˜ì˜€ìŠµë‹ˆë‹¤.
    

- **MinCUT regularization**
    
    : $L_{cut}=-\frac{Tr[S^TAS]}{Tr[S^TDS]}+\|\frac{S^TS}{\|S^TS\|_{F}}-\frac{I}{\sqrt{2}}\|_{_F}$ìœ¼ë¡œ í‘œí˜„ë˜ëŠ” Mincut problemì˜ ë¡œìŠ¤ í•¨ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤.
    
    : `mincut_loss`ëŠ” $-\frac{Tr[S^TAS]}{Tr[S^TDS]}$ ë¶€ë¶„ì„ `ortho_loss`ëŠ” $\|\frac{S^TS}{\|S^TS\|{F}}-\frac{I}{\sqrt{2}}\|_{_F}$ ë¶€ë¶„ì„ ê°ê° í‘œí˜„í•©ë‹ˆë‹¤.
    

## ğŸ“šÂ ì°¸ê³ ë¬¸í—Œ

---

[1] Arnaiz-RodrÃ­guez, AdriÃ¡n, Ahmed Begga, Francisco Escolano, and Nuria M. Oliver. (2022). â€˜DiffWire: Inductive Graph Rewiring vi33a the LovÃ¡Sz Boundâ€™. InÂ *Proceedings of the First Learning on Graphs Conference*, edited by Bastian Rieck and Razvan Pascanu, 198:15:1-15:27. 

[2] von Luxburg, U. (2007). A tutorial on spectral clustering.Â *Stat Comput*Â **17**, 395â€“416. 

[3] Gallier, Jean H. (2022). â€œGraph Clustering Using Ratio Cuts,â€ CIS 5105: Fundamentals of Linear Algebra and Optimization (class lecture, University of Pennsylvania, PA, US, Fall, 2022).

[4] Kang, Jian, and Hanghang Tong. (2019). â€˜N2N: Network Derivative Miningâ€™. InÂ *Proceedings of the 28th ACM International Conference on Information and Knowledge Management (CIKM)*, 861â€“70.
