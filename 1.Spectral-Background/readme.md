# Spectral-Background

`**Introduction**`

ë³¸ ì„¹ì…˜ì—ì„œëŠ” Graph rewiring ì‚¬ìš©ë˜ëŠ” ê¸°ë³¸ ê°œë…ì¸ Spectral Backgroundì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ë³¸ íŠœí† ë¦¬ì–¼ì˜ í•µì‹¬ì„ ë‹¤ë£¬ë‹¤ê¸°ë³´ë‹¤, 1.ì™œ ì´ëŸ° ë¬¸ì œê°€ ì œê¸°ë˜ì—ˆëŠ”ì§€ 2.ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ íŠœí† ë¦¬ì–¼ ì €ìë“¤ì€ ì–´ë–¤ ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼í–ˆëŠ”ì§€ 3.ì†”ë£¨ì…˜ ì•„ì´ë””ì–´ë¥¼ êµ¬í˜„í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ì¬ë£Œ ì´ 3ê°€ì§€ì— ëŒ€í•´ ì‚´í´ë³´ëŠ”ê²Œ ë³¸ ì„¹ì…˜ì˜ ì£¼ ëª©ì ì…ë‹ˆë‹¤.

## Overview

- GNN ê³ ì§ˆì ì¸ ë¬¸ì œ â€˜over-smoothingâ€™ , â€˜over-squashingâ€™ì— ëŒ€í•´ ì•Œì•„ë´…ë‹ˆë‹¤.
- Graph Rewiring ì— ëŒ€í•´ ì•Œì•„ë³´ë©°, ê³¼ì—° ì´ ë°©ë²•ë¡ ì´ ì •ë³´ ì „ë‹¬ ë¹„ëŒ€ì¹­ì„±ì„ ê°œì„ í•˜ëŠ”ë° ë„ì›€ì´ ë˜ëŠ”ê°€ì— ëŒ€í•´ì„œ ì•Œì•„ë´…ë‹ˆë‹¤.
- Diffwire ì˜ í•µì‹¬ ìš”ì†Œì¸ CT-LAYER, GAP-LAYERì˜ ì¬ë£Œë“¤ì´ ë¬´ì—‡ì¸ì§€ ì•Œì•„ë´…ë‹ˆë‹¤.

---

`Motivation`

over-squashing ì˜ í˜„ìƒì„ rewiring ìœ¼ë¡œ í•˜ëŠ”ê²Œ ë³¸ íŠœí† ë¦¬ì–¼ ì•„ì´ë””ì–´ì˜ í•µì‹¬ì…ë‹ˆë‹¤. graph topology ì— ë¹„ë¡€í•˜ì—¬ message-passingì´ ë°œìƒí•˜ê³ , ê·¸ì—ë”°ë¼ ì •ë³´ì „ë‹¬ì´ ë°œìƒí•©ë‹ˆë‹¤. ì´ë•Œ ì •ë³´ì „ë‹¬ì„ ì–´ë–¤ì‹ìœ¼ë¡œ í•˜ëŠ”ì§€ì— ë”°ë¼ GNNì˜ êµ¬ì¡°ê°€ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ë³¸ ì„¹ì…˜ì—ì„œëŠ” GNN ëŒ€í‘œì ì¸ ëª¨ë¸ 3ê°€ì§€ë¥¼ ì‹œì‘ìœ¼ë¡œ ë ˆì´ì–´ ìˆ˜ì— ë”°ë¼ ì–¼ë§ˆë‚˜ ì„±ëŠ¥ì´ ë‹¬ë¼ì§€ëŠ”ê°€ì— ëŒ€í•œ ì´ì•¼ê¸°ë¥¼ í•´ë³´ê² ìŠµë‹ˆë‹¤.

# GNN principle

â†’ ì—°ê²°ëœ ë…¸ë“œë“¤ë¡œë¶€í„° Message Passing ì„ ë°›ì•„ Aggregation í•œ ì •ë³´ì˜ Smoothnessë¥¼ ê¸°ë°˜ìœ¼ë¡œ weight ê°€ ê°±ì‹ ë¨. ì¦‰, graph feature ì™€ graph topology ë‘ ê°€ì§€ì— ë”°ë¼ weight ê°€ ë³€í•˜ë©°, í•´ë‹¹ weightë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì¶”ë¡ ì„ ì§„í–‰í•¨.

![Thomas, Josephine M., et al. "Graph neural networks designed for different graph types: A survey."Â *arXiv preprint arXiv:2204.03080*Â (2022).](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/bf54f5ce-c5df-4b45-a831-35ac4d405d0d/Untitled.png)

Thomas, Josephine M., et al. "Graph neural networks designed for different graph types: A survey."Â *arXiv preprint arXiv:2204.03080*Â (2022).

í¬ê²Œ convolutional , attentional , message-passingìœ¼ë¡œ ë‚˜ë‰˜ë©°, ë³¸ íŠœí† ë¦¬ì–¼ì—ì„  Message-passing ì˜ ê³µì‹ì„ í™œìš©í•œ MPNN ë ˆì´ì–´ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì§„í–‰í•¨.

**ë‹¨ìˆœí•˜ê²Œ ì ‘ê·¼í•´ë³´ë©´, ì •ë³´ê°€ ë§ì„ìˆ˜ë¡ ì¶”ë¡ ì„±ëŠ¥ì´ ë†’ì•„ì§„ë‹¤ ë¼ëŠ” ìƒê°ì„ í•  ìˆ˜ ìˆê¸°ì— GNN layerë¥¼ ì—¬ëŸ¬ê°œ ìŒ“ì. ë¼ëŠ” ìƒê°ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê³¼ì—° ê·¸ëŸ´ê¹Œìš”?**

# GNN layer increasing are good or bad?

ê·¸ë˜í”„ í† í´ë¡œì§€(êµ¬ì¡°)ë¥¼ ì¸¡ì •í•œ ë’¤, ì´ë¥¼ ê³ ë ¤í•´ì„œ GNN ì¸µì„ ìŒ“ëŠ”ê²Œ í•©ë¦¬ì ì¸ í”„ë¡œì„¸ìŠ¤ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°”ë¡œ ê·¸ë˜í”„ ë°ì´í„°ì˜ ê³ ìœ ì„±ê²©ì¸ â€˜ì—°ê²°â€™ì´ ë°˜ì˜ëœ ë°ì´í„°ë¼ëŠ” íŠ¹ì„± ë•Œë¬¸ì¸ë°ìš”. íƒ€ ë°ì´í„°(tabular , image , text) ë“¤ê³¼ëŠ” ë‹¤ë¥´ê²Œ ë°ì´í„° ë‹¤ìˆ˜ê°€ power-law ë¶„í¬ë¥¼ ë•ë‹ˆë‹¤. ë‹¤ì‹œ ë§í•´ì„œ, ë¹ˆìµë¹ˆ ë¶€ìµë¶€ ë°ì´í„°ë“¤ì´ ë§ìŠµë‹ˆë‹¤. íŠ¹ì • ë…¸ë“œì— ë§ì€ ì—£ì§€ê°€ ì—°ê²°ë˜ì–´ ìˆë‹¤ë©´, ê·¸ì— ë¹„ë¡€í•˜ì—¬ ë§ì€ ì •ë³´ëŸ‰ì´ ì „ì†¡ë˜ì–´ íŠ¹ì • ë…¸ë“œë§Œì˜ ê³ ìœ  ì •ë³´ë¥¼ ìƒê²Œ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ëŒ€ë¡œ, íŠ¹ì • ë…¸ë“œì— ì ì€ ì—£ì§€ê°€ ì—°ê²°ë˜ì–´ ìˆë‹¤ë©´ ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ì •ë³´ë§Œ ì „ì†¡ë°›ê¸°ì— íŠ¹ì • ë…¸ë“œë§Œì˜ ê³ ìœ  ì •ë³´ê°€ ì§€ë°°ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì¶”ë¡ ì‹œì— ë‹¤ë¥¸ ë…¸ë“œë“¤ì˜ ì •ë³´ê°€ ë§ì´ ë°˜ì˜ë˜ì—ˆê±°ë‚˜, ë³¸ì¸ ë…¸ë“œë§Œì˜ ì •ë³´ê°€ ë§ì´ ë°˜ì˜ë˜ì—ˆê±°ë‚˜ ë‘˜ ì¤‘ í•˜ë‚˜ì˜ ê²½ìš°ê°€ ë°œìƒí•˜ê²Œ ë©ë‹ˆë‹¤. ê²°êµ­ â€˜ì ì ˆí•œâ€™ ì •ë³´ ì „ë‹¬(message passing)ì´ í•µì‹¬ì´ë©°, ì´ë¥¼ ê°œì„ í•˜ê³ ì ë‹¤ì–‘í•œ ì—°êµ¬ë“¤ì´ ì‹œë„ë˜ê³  ìˆìŠµë‹ˆë‹¤. 

ê°œì¸ì ìœ¼ë¡œëŠ” GNN ëª¨ë¸ë§ ì „, ë‹¤ìŒ 3ê°€ì§€ ë¶„í¬ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì¸¡ì •í•´ë³´ì‹  í›„ ì˜ì‚¬ê²°ì •ì„ í•˜ì‹œëŠ”ê±¸ ì¶”ì²œí•©ë‹ˆë‹¤.

1. graph diameter(ê·¸ë˜í”„ êµ¬ì¡° í¬ê¸°) , 2. degree distribution(ì—°ê²° ë¶„í¬) , 3. assortativity coefficient(ë ˆì´ë¸” ë¶„í¬)

ìœ„ ë‚´ìš©ì— ëŒ€í•´ ë” ê¶ê¸ˆí•˜ì‹œë‹¤ë©´, ì•„ë˜ ë…¼ë¬¸ì„ ì°¸ì¡°í•˜ì‹œë©´ ë§ì€ ë„ì›€ì´ ë  ê±°ë¼ ìƒê°í•©ë‹ˆë‹¤.

Deac, Andreea, Marc Lackenby, and Petar VeliÄkoviÄ‡. "Expander graph propagation."Â *Learning on Graphs Conference*. PMLR, 2022. ****[[paper link](https://proceedings.mlr.press/v198/deac22a/deac22a.pdf)] 

![Alon, U. and Yahav, E. â€œOn the bottleneck of graph neural networks and its practical implicationsâ€. In ICLR 2021](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/164f8188-876e-4576-b7b8-3eb192102e6e/Untitled.png)

Alon, U. and Yahav, E. â€œOn the bottleneck of graph neural networks and its practical implicationsâ€. In ICLR 2021

where *k* = n layers , *r* = radius , *d* = graph diameter. ê·¸ë˜í”„ ë³¸ì§ˆì ì¸ íŠ¹ì„±ì„ íŒŒì•…í•´ë³´ê³ ,ê·¸ì— íƒ€ë‹¹í•œ ë ˆì´ì–´ë¥¼ ìŒ“ì•„ì•¼ íš¨ìœ¨ì ì¸ í•™ìŠµì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” *r*,*d* ê·¸ë˜í”„ ì§€ë¦„ì„ ê·¸ ì§€í‘œë¡œ í™œìš©í•©ë‹ˆë‹¤. **(over-smoothing)** ì´ë¥¼ ê³ ë ¤ì¹˜ ì•Šê³  ë§‰ë¬´ê°€ë‚´ë¡œ ë ˆì´ì–´ë¥¼ ì¦ê°€(k)ì‹œí‚¨ë‹¤ë©´ ì˜¤íˆë ¤ smoothness ê°€ ê³¼ë„í•˜ê²Œ ì¦ê°€í•˜ì—¬ ë…¸ë“œë“¤ì„ êµ¬ë³„í•˜ê¸° ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤. ì¦‰, message passing ìœ¼ë¡œ ì „ë‹¬ëœ ì •ë³´ë“¤ì´ ë‹¤ì–‘ì„±ì´ ì ì–´ì§€ê¸°ì— generalization ì—ì„œ ë¶ˆë¦¬í•œ ìƒí™© ë°œìƒí•˜ëŠ” ê±°ì£ . **(over-squashing)** ë˜í•œ , ê·¸ë˜í”„ ë‚´ íŠ¹ì • ë…¸ë“œ(bottleneck)ì— ê³¼ë„í•œ ì •ë³´ê°€ ì „ë‹¬ë˜ì–´ ê·¸ë˜í”„ ë‚´ ì •ë³´êµë¥˜ê°€ ê³ ë£¨ ë¶„í¬ë˜ì§€ ì•ŠìŒ.

> To capture long-range dependencies between nodes, GNNs must perform at least as many message-passing steps (i.e., have as many layers) as the distance between node pairs to not incur in under-reaching. However, building deep GNNs presents an inherent challenge. As depth increases, the receptive field of nodes grows exponentially, thus requiring more information to be encoded in the same fixed-size vectors.
> 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b2871568-6fad-4945-af56-fbf06dde9f52/Untitled.png)

> Over-squashing arises when the derivative in above equation result becomes too small, indicating that the representation of node v is mostly insensitive to the information initially present at node u. While increasing the layer Lipschitz constants or the dimension H can mitigate the issue , this may come at the expense of model generalization . Therefore, different methods have been proposed to alter the graph topology in a more favorable way to message-passing.
> 

- Lipschitz constants : it measures the rate of change of a function as we move between adjacent vertices in the graph.

---

`**Background**`

# Rewiring methods

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9ed110c6-ab16-45be-af91-4f9be47568bc/Untitled.png)

**Heat** **kernel**[diffusion based] 

A ëŠ” Transition matrix ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.  parameter ë¡œ heat equation(t) ë¥¼ í™œìš©í•©ë‹ˆë‹¤. 

$$
M_{heat} = e^{-t\mathbf{A}}.
$$

**Threshold**

$$
\theta^{Heat}_{m}=e^{-t}\frac{1}{m!} \ ,t>0.
$$

ì‹œê°„ì´ ë³€í•¨ì— ë”°ë¼ information flow ê°€ ì–´ë–»ê²Œ ë³€í•˜ëŠ”ì§€ ì¸¡ì •í•˜ê¸° ìœ„í•´ heat ìš”ì†Œë¥¼ threshold ë¡œ í™œìš©í•©ë‹ˆë‹¤. 

**PageRank** [diffusion based] 

A ëŠ” Transition matrix ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. parameter ë¡œ \alpha ë¥¼ í™œìš©í•©ë‹ˆë‹¤. 

$$
M_{PageRank} = \alpha(1 âˆ’ (1 âˆ’ \alpha)\mathbf{A}^+.
$$

**Threshold**

$$
\theta^{Pagerank}_{m}=\alpha(1-\alpha)^m,0 < \alpha < 1.
$$

\alphaë¥¼ ì‚¬ì „ì— ì§€ì •í•´ì¤ë‹ˆë‹¤. íŠ¹ì • ë…¸ë“œê°€ ì—°ê²°ëœ íƒ€ ë…¸ë“œë¡œ ì´ë™í• ê²ƒì¸ì§€ì— ëŒ€í•œ ì—¬ë¶€ë¥¼ í™•ë¥ ì ìœ¼ë¡œ ê³„ì‚°í•  ë•Œ \alpha ê°’ì— ì˜í–¥ì„ ë°›ìŠµë‹ˆë‹¤. 

**SDRF** [curvature based] iteratively samples an edge (u, v) proportionally to how negatively curved it is, *then adds the new edge (uâ€², vâ€²) able to provide the largest increase of Ricuv*. (The algorithm optionally removes the most positively curved edges to avoid growing the graph excessively.)

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ce2def8e-ee95-474b-9d55-f682c78ffe56/Untitled.png)

**GRLEF** [curvature based] proposes to improve a graph spectral gap by working exclusively locally via the triangle counts â™¯â–³uv, which are cheaper to compute as they require just neighborhood information. The algorithm iteratively samples an edge (u, v) proportionally to the inverse of triangle count, that is from an area of the graph that is locally far away from being fully-connected. Then *it chooses the pair of edges (u, uâ€²),(v, vâ€²) to flip into (u, vâ€²),(v, uâ€²) which provides the smallest net change in triangle count. This behavior can be interpreted as mitigating a very low local curvature* (as suggested by the small term â™¯â–³uv in Ricuv) at the expense of a reduction in curvature of more positively curved neighboring edges. Banerjee et al. [7] supported the approach of their rewiring algorithm by empirically finding a correspondence between triangle count decrease and spectral gap increase.

![GRLEF algorithm](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7a73abb8-301a-4e28-af1e-c10b9626a687/Untitled.png)

GRLEF algorithm

**EGP** [curvature based & messpage passing] are derived from the Cayley graphs of finite groups SL(2, Zn), which are 4-regular and thus guarantee sparsity. Interestingly, these graphs have all negatively curved edges with Ric_uv = âˆ’1/2. In our experiments we will thus use the message-passing matrix M_EGP = A_Cay A, where A_Cay is the adjacency matrix of said Cayley graphs.

![cayley graph](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c2dca2c7-ad66-44e0-b94b-30a9c43b8e8d/Untitled.png)

cayley graph

![EGP algorithm](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/ae18d782-4fb0-4d2e-a180-62ff5df55ca4/Untitled.png)

EGP algorithm

**DiffWire**(Hybrid , curvature and diffusion) , minimize the structure transformation but keep the information by sparsification(using the CT , GAP layer). 

> Our aim is to leverage **diffusion and curvature theories** to propose a new approach for graph rewiring that preserves the graphâ€™s structure.
> 

DiffWire â€˜diffusionâ€™ and â€˜curvatureâ€™ theories ì˜ ê¸°ë³¸ì´ ë˜ëŠ” ê°œë… spectral theory ì— ëŒ€í•´ ì•Œì•„ì•¼í•©ë‹ˆë‹¤. ìœ„ì— ë‚˜ì—´ëœ ë°©ë²•ë¡ ë“¤ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª…ì€ 2ï¸âƒ£Â Transductive rewiring ì—ì„œ ì´ì•¼ê¸° í•˜ê² ìŠµë‹ˆë‹¤.

---

# Bottleneck measurement

Sparifiaction ë¥¼ ìœ„í•´ì„œ ê·¸ë˜í”„ êµ¬ì¡° ì¤‘ ì–´ëŠ ë…¸ë“œì—ì„œ bottleneck ì´ ë°œìƒí•˜ëŠ”ì§€ íŒŒì•…í•´ì•¼í•©ë‹ˆë‹¤. í¬ê²Œ local bottleneck, global bottleneck measurement ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. 

* Sparsification refers to the process of transforming a given graph into a sparser graph while preserving certain structural properties or characteristics. A sparser graph has fewer edges than the original graph, which can be beneficial for various computational and analytical purposes.

**Local bottleneck**

Intuitively, for a tree the receptive field grows exponentially in the branching factor, while at the other opposite a complete graph has a constant receptive field. To provide a metric to quantify this local behavior, they have proposed the balanced Forman curvature, defined as

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/b6297473-e863-4119-a24c-dd59d4108298/Untitled.png)

SDRF [52] iteratively samples an edge (u, v) proportionally to how negatively curved it is, then adds the new edge (uâ€², vâ€²) able to provide the largest increase
of Ricuv. (The algorithm optionally removes the most positively curved edges to avoid growing the graph excessively.)

**Global bottleneck**

A more global metric is the **Cheeger constant** â€˜h_Gâ€™, which quantifies the minimum fraction of edges that need to be removed in order to make the graph disconnected. A small â€˜h_Gâ€™ thus indicates that few edges act as a bridge between two otherwise disconnected communities. However, computing the Cheeger constant is an NP-hard problem, so the lower bound given by the spectral gap Î»1 (i.e. the smallest positive Laplacian eigenvalue) is used as a proxy measure in practice: hG â‰¥ 1/2 Î»1 . 

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/e0a4be52-e5d9-409a-a88f-099f812e406f/Untitled.png)

> â†’ having a small Cheeger constant is equivalent to the graph having a â€˜bottleneckâ€™. in the sense that there is a collection of edges BA that, when removed, disconnects the vertices into two sets (A and its complement, V pGqzA), with the property that the sizes of A and its complement are significantly larger than the size of BA. -Expander grpah propagation.
> 

**Effective resistance.**

> Effective resistance [14] provides an additional way to measure bottlenecks in graph topology. **The resistance Res_uv between two nodes is proportional to the commute time Com_uv, which is the number of expected steps for a random walk to go back and forth between nodes u, v. An high resistance between two nodes is an indication of the difficulty for messages to pass from node u to node v.** Black et al. [9] proved a sensitivity bound similar to (2) relating high effective resistance Resvu between pairs of nodes to a reduced sensitivity of the representations h(L) v to input features xu. Furthermore, effective resistance is inversely related to the square of the Cheeger constant by the inequality max(u,v)âˆˆE Res_uv â‰¤ 1h2G[4]. ***Arnaiz-RodrÃ­guez et al. [4] have proposed a layer for learning effective resistance to re-weight the original graph adjacency (hence â€˜DiffWireâ€™) in the perspective of sampling a spectrally similar but sparser graph which preserves the graph structural information [47].*** The additional intuitive effect is to enlarge the relative capacity of high resistance edges, which correspond to bridges over more densely connected communities. In our experiments we implement the DiffWire approach by computing the effective resistance in exact form by Resuv = (1u âˆ’ 1v)âŠ¤L+(1u âˆ’ 1v) with 1u the indicator vector of node u. The resulting message-passing matrix therefore is MDiffWire = Res âŠ™ A, where â€˜âŠ™â€™ denotes the elementwise product.
> 

---

# Bottleneck solution , curvature vs. diffusive.

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/efc6c9e6-7140-4e40-ace7-9ae093dd7b1a/Untitled.png)

- diffusion works better in homophilic graphs. Needs parameters \alpha (or t) and \epsilon
- SDRF works better in hetrophilic graphs Needs parameters \tau and C^+

â†’ ìœ„ ë‘ ë°©ë²• ëª¨ë‘ â€˜parameterâ€™ê°€ í•„ìš”í•¨. ë‹¤ë¥´ê²Œ ë§í•˜ë©´ parameter ì— ë”°ë¼ ì„±ëŠ¥ ì˜ì¡´ì„±ì´ ê°•í•˜ë¯€ë¡œ unseen task ì¸ generalization performance ì—ì„œì˜ ì„±ëŠ¥ì„ ë³´ì¥í•˜ê¸°ì—ëŠ” ì–´ë µë‹¤. ë¼ê³  ë³¼ ìˆ˜ ìˆìŒ. í•˜ì§€ë§Œ ë³¸ ë°©ë²•ë¡  DiffwireëŠ” Parameter-free methodology ê°€ëŠ¥í•¨.

---

## How?

â€˜The LovÃ¡sz Boundâ€™

the LovÃ¡sz bound is a mathematical concept that provides an upper bound on the chromatic number of a graph. It is obtained by solving a linear program.

ì—„ë°€í•œ ìˆ˜í•™ì  ìœ ë„ì‹ê³¼ ì„¤ëª…ì„ ì›í•˜ì‹œëŠ” ë¶„ì€ [https://www.cs.cmu.edu/~15859n/RelatedWork/random-walks-on-graphs.pdf] ë¥¼ ì¶”ì²œë“œë¦½ë‹ˆë‹¤.

![left ; CT-LAYER , right : GAP-LAYER.](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/4ab6cb8e-8614-409b-8e98-7c9211cfecc0/Untitled.png)

left ; CT-LAYER , right : GAP-LAYER.

CT-LAYER : as **a metric-based** , neural approach.

â†’ a layer that learns **the commute times** and **uses them as a relevance function for edge re-weighting.**

![**Graph Rewiring: From theory to Applications in Fairness , page 50/n**](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/7ffca247-8b76-430d-aea8-48b495dd2ff5/Untitled.png)

**Graph Rewiring: From theory to Applications in Fairness , page 50/n**

GAP-LAYER : as **a direct-neural approach to optimize the structure** of the graph to the task at hand.

â†’ a layer to **optimize the spectral gap**, depending on the nature of the network and the task at hand. 

compute these gradient either using Laplacians(L, with Fiedler \lambda_2) or normalized Laplacians (L, with Fiedler \lambda_2^â€™).

- Fiedler Vector
    - Courant-Fisher Theoremì„ í†µí•´ , self-adjoint matrix ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ì´ ë•Œ ëª©ì ì€ â€˜**orthogonalâ€™** ë¥¼ ì¶”ì¶œí•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ë˜í•œ, Rayliegh Quotient ë¥¼ í™œìš©í•˜ì—¬ ì£¼ì–´ì§„ ë²¡í„°ê°€ í–‰ë ¬ Aì˜ ì‘ìš© í•˜ì—ì„œ ì–´ë–»ê²Œ "ëŠ˜ì–´ì§€ê±°ë‚˜" ìˆ˜ì¶•í•˜ëŠ”ì§€ì— ëŒ€í•´ íŒŒì•…í•˜ì—¬ eigenvalue ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.
        
        ![**Graph Rewiring: From theory to Applications in Fairness , page 20/n**](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/60a040cc-b0bc-4b2f-8581-977ca23a36f7/Untitled.png)
        
        **Graph Rewiring: From theory to Applications in Fairness , page 20/n**
        
- Average Cut Problem
    - ì•ì„œ ì–¸ê¸‰í•œ fiedler vector ëŠ” ì•„ë˜ average cut problem ì—ì„œ í™œìš©ë©ë‹ˆë‹¤. ìµœì ì˜ partition ì„ ìœ„í•´ ê° ë…¸ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ cutting ì„ í•˜ëŠ”ë°, ëª¨ë“  ë…¸ë“œ ì¡°í•©ì„ ê³ ë ¤í•´ì•¼í•˜ê¸°ì— NP-Hard ë¬¸ì œì— ì†í•©ë‹ˆë‹¤. ì´ë¥¼ fielder vector ë¥¼ í†µí•´ í•´ê²°í•©ë‹ˆë‹¤.
    
    ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/78bf0369-165e-462a-8638-509a63f35a91/Untitled.png)
    

** Fiedler vector vs. eigen vector ì°¨ì´ì .

eigenvector ì˜ í•˜ìœ„ ìš”ì†Œê°€ Fiedler vector ì´ë©°, optimal partition ì„ ìœ„í•œ ì¬ë£Œë¡œ í™œìš©ë©ë‹ˆë‹¤. 

** Green's Function:

In mathematics and physics, Green's function is a mathematical tool used to solve certain types of differential equations. It provides a way to find the response of a system to an impulse or a localized source.

---

`**Code with explanation**`

# Graph Laplacian

ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬(Graph Laplacian matrix)ì€ ê·¸ë˜í”„ ì´ë¡ ì—ì„œ ì¤‘ìš”í•œ ê°œë… ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬ì€ ì£¼ì–´ì§„ ê·¸ë˜í”„ì˜ êµ¬ì¡°ì ì¸ íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì •ë°© í–‰ë ¬(asymmetric matrix)ì…ë‹ˆë‹¤. ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬ì€ ê·¸ë˜í”„ì˜ ì •ì ë“¤ê³¼ ê°„ì„ ë“¤ì˜ ì—°ê²°ì„±ì„ í‘œí˜„í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ, ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬ì€ ê·¸ë˜í”„ì˜ ì¸ì ‘ í–‰ë ¬(A)ê³¼ ì°¨ìˆ˜ í–‰ë ¬(D)ì˜ ì°¨ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì¸ì ‘ í–‰ë ¬ì€ ê·¸ë˜í”„ì˜ ì •ì  ê°„ì˜ ì—°ê²° ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚´ë©°, ì°¨ìˆ˜ í–‰ë ¬ì€ ê° ì •ì ì˜ ì°¨ìˆ˜(ì—°ê²°ëœ ê°„ì„ ì˜ ìˆ˜)ë¥¼ ëŒ€ê° í–‰ë ¬ë¡œ í‘œí˜„í•©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤:

$$
L = D - A
$$

ì—¬ê¸°ì„œ Lì€ ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬ì„ ë‚˜íƒ€ë‚´ë©°, DëŠ” ì°¨ìˆ˜ í–‰ë ¬, AëŠ” ì¸ì ‘ í–‰ë ¬ì…ë‹ˆë‹¤. ì˜ˆì‹œ ê·¸ë¦¼ì„ í†µí•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

![redraw.](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/93999196-78ee-4f41-89d7-09adee4a88db/Untitled.png)

redraw.

## code snippet

```python
A = nx.adjacency_matrix(G)
L = nx.laplacian_matrix(G)

e, evecs = np.linalg.eig(L.todense())
idx = e.argsort()
e = e[idx]
evecs = evecs[:,idx]
```

# Dirichlet energy graph

ì•ì„œ ë„ì¶œí•œ ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆê³¼ ë³¸ ì„¹ì…˜ì—ì„œ ë‹¤ë£¨ì–´ë³¼ ë””ë¦¬í´ë ˆ ì—ë„ˆì§€ëŠ” ë¬´ìŠ¨ ê´€ë ¨ì´ ìˆì„ê¹Œìš”?

ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆê³¼ ë””ë¦¬í´ë ˆ ì—ë„ˆì§€ëŠ” ê·¸ë˜í”„ ì´ë¡ ê³¼ ìŠ¤í™íŠ¸ëŸ´ ê·¸ë˜í”„ ì´ë¡  ë¶„ì•¼ì—ì„œ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆëŠ” ê°œë…ì…ë‹ˆë‹¤. ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆì€ ê·¸ë˜í”„ì˜ ì¤‘ìš”í•œ êµ¬ì¡°ì  íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” í–‰ë ¬ í‘œí˜„ì´ë©°, ë””ë¦¬í´ë ˆ ì—ë„ˆì§€ëŠ” ê·¸ë˜í”„ ìƒì˜ ë§¤ë„ëŸ¬ì›€ì´ë‚˜ ê·œì¹™ì„±ì„ ì¸¡ì •í•˜ëŠ” ì²™ë„ì…ë‹ˆë‹¤. ê·¸ë˜í”„ì˜ ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬ì€ nê°œì˜ ì •ì ì„ ê°–ëŠ” ë¬´ë°©í–¥ ê·¸ë˜í”„ì— ëŒ€í•´ L = D - Aë¡œ ì •ì˜ë©ë‹ˆë‹¤. ì—¬ê¸°ì„œ DëŠ” ëŒ€ê°ì„±ë¶„ì´ ì •ì ì˜ ì°¨ìˆ˜ì¸ ëŒ€ê° ì°¨ìˆ˜ í–‰ë ¬ì´ê³ , AëŠ” ê·¸ë˜í”„ì˜ ì¸ì ‘ í–‰ë ¬ì…ë‹ˆë‹¤. **ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬ì€ ê·¸ë˜í”„ì˜ ì—°ê²°ì„±ê³¼ ìœ„ìƒì— ëŒ€í•œ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤.** í•œí¸, **ë””ë¦¬í´ë ˆ ì—ë„ˆì§€ëŠ” ê·¸ë˜í”„ ìƒì˜ ì •ì ì— ì •ì˜ëœ í•¨ìˆ˜ê°€ ìƒìˆ˜ì—ì„œ ì–¼ë§ˆë‚˜ ë²—ì–´ë‚˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ì²™ë„**ì…ë‹ˆë‹¤.

$$
E(f) = \frac{1}{2} \sum_{i,j}\in E , (f(i)-f(j))^2
$$

ì—¬ê¸°ì„œ (i, j)ëŠ” ê·¸ë˜í”„ì˜ ê°„ì„ ì„ ë‚˜íƒ€ë‚´ê³ , w_ijëŠ” ê·¸ ê°„ì„ ì— ì—°ê²°ëœ ê°€ì¤‘ì¹˜ì´ë©°, f(i)ì™€ f(j)ëŠ” ê°ê° ì •ì  iì™€ jì—ì„œ í•¨ìˆ˜ì˜ ê°’ì…ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë””ë¦¬í´ë ˆ ì—ë„ˆì§€ëŠ” **ê·¸ë˜í”„ ìƒì—ì„œ í•¨ìˆ˜ì˜ ë§¤ë„ëŸ¬ì›€ì´ë‚˜ ê·œì¹™ì„±ì„ ì¸¡ì •**í•©ë‹ˆë‹¤. ì •ë¦¬í•´ë³´ìë©´, ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬ì€ ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  íŠ¹ì„±ì„ í¬ì°©í•˜ëŠ” ë°˜ë©´, ë””ë¦¬í´ë ˆ ì—ë„ˆì§€ëŠ” ê·¸ë˜í”„ì— ì •ì˜ëœ í•¨ìˆ˜ì˜ ë¶€ë“œëŸ¬ì›€ ë˜ëŠ” ê·œì¹™ì„±ì„ ì¸¡ì •í•©ë‹ˆë‹¤. **ë¼í”Œë¼ì‹œì•ˆ í–‰ë ¬ì˜ ê³ ìœ  ë²¡í„°ëŠ” ì´ëŸ¬í•œ ê³ ìœ  ë²¡í„°ì— ëŒ€í•œ í•¨ìˆ˜ì˜ ì œê³± íˆ¬ì˜ì˜ ê´€ì ì—ì„œ ë””ë¦¬í´ë ˆ ì—ë„ˆì§€ë¥¼ í‘œí˜„í•˜ê¸° ìœ„í•œ ê¸°ì´ˆë¥¼ ì œê³µ**í•©ë‹ˆë‹¤.

**ì°¸ê³ ë¡œ, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” rewiring ì „/í›„ fiedler vector ê°„ dirichlet energy ë¥¼ ì¸¡ì •í•˜ì—¬, ìµœì ì˜ sparification ì„ ì§„í–‰í•©ë‹ˆë‹¤.**

## code snippet

```python
#1
idxAll = set(range(G.number_of_nodes()))
idxB = set([0, 19])
idxD = set([x for x in idxAll if x not in idxB])
# print(idxAll)

idxB = np.array(list(idxB))
idxD = np.array(list(idxD))
# print(idxB)
# print(idxD)

Ldense = L.todense()
print(Ldense)
LdenseD = Ldense[idxD,:]
LdenseD = LdenseD[:,idxD]

print(f"LdenseD matrix{LdenseD}")
print(idxD.shape[0])
print(idxB.shape[0])
print(np.zeros((2,2)))

Bdense = np.zeros((idxB.shape[0],idxD.shape[0]))
print(Ldense.shape)
Bdense = Ldense[idxB,:]
Bdense = Bdense[:,idxD]
print(Bdense)

Inv = np.matmul(np.linalg.inv(LdenseD),-np.transpose(Bdense))
print("inv",Inv)
uB = np.array([1,0])
uD = np.matmul(Inv,np.transpose(uB))
print(uD)
print(uD.shape)

#2
def calculate_dirichlet_energy(G, u):
    """
    Calculate the Dirichlet energy of a function u defined on a graph G.
    
    Parameters:
        G (NetworkX Graph): The graph on which the function is defined.
        u (dict): A dictionary mapping node IDs to function values.
        
    Returns:
        float: Dirichlet energy of the function.
    """
    energy = 0.0
    
    for node in G.nodes():
        neighbors = list(G.neighbors(node))
        degree = G.degree(node)
        u_node = u[node]
        
        for neighbor in neighbors:
            u_neighbor = u[neighbor]
            energy += (u_neighbor - u_node)**2
        
        energy += degree * u_node**2
    
    return energy
```

# Heat Kernel and graph diffusion

íˆíŠ¸ ì»¤ë„(heat kernel)ê³¼ ê·¸ë˜í”„ í™•ì‚° ëª¨ë¸(graph diffusion model)ì€ ëª¨ë‘ ê·¸ë˜í”„ ìƒì—ì„œì˜ í™•ì‚° í˜„ìƒê³¼ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. íˆíŠ¸ ì»¤ë„ê³¼ ê·¸ë˜í”„ í™•ì‚° ëª¨ë¸ì€ **ì •ë³´ë‚˜ ì—´ì´ ê·¸ë˜í”„ì˜ ì •ì ì„ í†µí•´ ì–´ë–»ê²Œ ì „íŒŒ**ë˜ëŠ”ì§€ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤. íˆíŠ¸ ì»¤ë„ì€ **ì—°ì†ì ì¸ ê³µê°„ì—ì„œ ì—´ì´ë‚˜ ì •ë³´ì˜ ì‹œê°„ì— ë”°ë¥¸ ë³€í™”ë¥¼ íŠ¹ì„±í™”**í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê·¸ë˜í”„ì— ì ìš©ë˜ëŠ” ê²½ìš°, íˆíŠ¸ ì»¤ë„ì€ ì •ì  ê°„ ì—°ê²°ì„±ì— ê¸°ë°˜í•˜ì—¬ ì—´ì´ë‚˜ ì •ë³´ê°€ ì–´ë–»ê²Œ í™•ì‚°ë˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ë°˜ë©´, ê·¸ë˜í”„ í™•ì‚° ëª¨ë¸ì€ íŠ¹íˆ ê·¸ë˜í”„ ìƒì—ì„œì˜ í™•ì‚° ê³¼ì •ì„ ë‹¤ë£¹ë‹ˆë‹¤.  **ì •ë³´ë‚˜ ì—´ì´ ê·¸ë˜í”„ì˜ ê°„ì„ ì„ í†µí•´ í™•ì‚°ë˜ëŠ” ê²ƒì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. í™•ì‚°ì´ ì¼ë°˜ì ìœ¼ë¡œ ì´ì‚°ì‹œê°„ ë§ˆë¥´ì½”í”„ ì²´ì¸ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ë§ë˜ë©°, ì •ì  ê°„ì˜ ì „ì´ í™•ë¥ ì€ ê·¸ë˜í”„ ê°„ì„ ì— ì—°ê²°ëœ ê°€ì¤‘ì¹˜ë‚˜ ìœ ì‚¬ë„ ì¸¡ì •ì¹˜ì— ë”°ë¼ ê²°ì •**ë©ë‹ˆë‹¤. íˆíŠ¸ ì»¤ë„ê³¼ ê·¸ë˜í”„ í™•ì‚° ëª¨ë¸ì˜ ìƒê´€ê´€ê³„ëŠ” **ì—°ì†ì ì¸ í™•ì‚°ê³¼ ì´ì‚°ì ì¸ í™•ì‚° ê³¼ì • ê°„ì˜ ê´€ê³„**ì— ìˆìŠµë‹ˆë‹¤. íˆíŠ¸ ì»¤ë„ì€ ê·¸ë˜í”„ì— ì ìš©ë˜ì–´ ì´ì‚°í™”ë  ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ê·¸ë˜í”„ í™•ì‚° ëª¨ë¸ê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. ì´ì‚°ì ì¸ ê·¸ë˜í”„ í™•ì‚° ê³¼ì •ì€ íˆíŠ¸ ì»¤ë„ì´ ê¸°ìˆ í•˜ëŠ” ì—°ì†ì ì¸ ì—´ í™•ì‚°ì„ ê·¼ì‚¬í™”í•œ ê²ƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## code snippet

```python
def heat_kernel(G,e,evecs,beta):
  n = G.number_of_nodes()
  H = np.zeros((n,n))
  Phi = evecs
  Lambda = np.diag(np.exp(-beta*e))
  H = np.matmul(Phi,Lambda)
  H = np.matmul(H,np.transpose(Phi))
  return H
```

# Commute Times

Commute TimesëŠ” ê·¸ë˜í”„ì˜ ë‘ ê°œì˜ ì •ì  ì§‘í•© ê°„ì— ëœë¤ ì›Œì»¤(Random walker)ê°€ í•œ ì§‘í•©ì—ì„œ ë‹¤ë¥¸ ì§‘í•©ìœ¼ë¡œ ì´ë™í•œ í›„ ë˜ëŒì•„ì˜¤ê¸°ê¹Œì§€ í‰ê· ì ìœ¼ë¡œ ê±¸ë¦¬ëŠ” ì‹œê°„ìœ¼ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œëŠ” **ê·¸ë˜í”„ ë¼í”Œë¼ì‹œì•ˆì˜ ë‘ ë²ˆì§¸ë¡œ ì‘ì€ ê³ ìœ ê°’ê³¼ ì„¸ ë²ˆì§¸ë¡œ ì‘ì€ ê³ ìœ ê°’ì˜ ì°¨ì´ì˜ ì—­ìˆ˜**ë¡œ ê³„ì‚°ë©ë‹ˆë‹¤.

ìŠ¤í™íŠ¸ëŸ´ ì»¤ë®¤íŠ¸ íƒ€ì„ì€ ë‘ ê°œì˜ ì •ì  ì§‘í•© ê°„ì˜ ì „ì´ì˜ ë‚œì´ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ë§Œì•½ **ìŠ¤í™íŠ¸ëŸ´ ì»¤ë®¤íŠ¸ íƒ€ì„ì´ í¬ë‹¤ë©´, ê·¸ê²ƒì€ ë‘ ê°œì˜ ì§‘í•©ì´ ì˜ ë¶„ë¦¬ë˜ê³  êµ¬ë³„ë¨**ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ, **ìŠ¤í™íŠ¸ëŸ´ ì»¤ë®¤íŠ¸ íƒ€ì„ì´ ì‘ë‹¤ë©´, ê·¸ê²ƒì€ ë‘ ê°œì˜ ì§‘í•©ì´ ìƒí˜¸ ì—°ê²°ë˜ê³  êµ¬ë³„í•˜ê¸° ì–´ë µë‹¤ëŠ” ê²ƒì„ ì‹œì‚¬**í•©ë‹ˆë‹¤.

ìŠ¤í™íŠ¸ëŸ´ ì»¤ë®¤íŠ¸ íƒ€ì„ì€ ê·¸ë˜í”„ ë¶„í• , ì»¤ë®¤ë‹ˆí‹° íƒì§€, í´ëŸ¬ìŠ¤í„°ë§ ì•Œê³ ë¦¬ì¦˜ ë“± ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤. ìŠ¤í™íŠ¸ëŸ´ ì»¤ë®¤íŠ¸ íƒ€ì„ì„ ë¶„ì„í•¨ìœ¼ë¡œì¨ ê·¸ë˜í”„ ë‚´ì—ì„œ ì¡°í™”ë¡œìš´ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ì‹ë³„í•˜ê±°ë‚˜ ê·¸ë˜í”„ë¥¼ ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ë¡œ ë¶„í• í•˜ëŠ” ìµœì ì˜ ì ˆë‹¨ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<aside>
ğŸ’¡ ëœë¤ ì›Œì»¤(Random walker)ë€ ê·¸ë˜í”„ ìƒì—ì„œ ì´ë™í•˜ëŠ” ê°€ìƒì˜ ê°œì²´ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ëœë¤ ì›Œì»¤ëŠ” íŠ¹ì • ì •ì ì—ì„œ ì‹œì‘í•˜ì—¬ ê·¸ë˜í”„ì˜ ë‹¤ë¥¸ ì •ì ë“¤ ì‚¬ì´ë¥¼ ë¬´ì‘ìœ„ë¡œ ì´ë™í•˜ë©´ì„œ ê·¸ë˜í”„ë¥¼ íƒìƒ‰í•©ë‹ˆë‹¤. ì´ë™ ê²½ë¡œëŠ” í˜„ì¬ ìœ„ì¹˜ì˜ ì´ì›ƒ ì •ì ë“¤ ì‚¬ì´ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„ íƒë©ë‹ˆë‹¤. ëœë¤ ì›Œì»¤ëŠ” ê·¸ë˜í”„ì˜ êµ¬ì¡°ì™€ ì—°ê²°ì„±ì„ íƒìƒ‰í•˜ê³  íŠ¹ì • ì •ì  ê°„ì˜ ì´ë™ íŒ¨í„´ì´ë‚˜ ì •ë³´ ì „íŒŒì˜ íŠ¹ì„±ì„ ì¡°ì‚¬í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ìŠ¤í™íŠ¸ëŸ´ ì»¤ë®¤íŠ¸ íƒ€ì„ì˜ ê²½ìš°, ëœë¤ ì›Œì»¤ëŠ” í•˜ë‚˜ì˜ ì •ì  ì§‘í•©ì—ì„œ ë‹¤ë¥¸ ì •ì  ì§‘í•©ìœ¼ë¡œ ì´ë™í•˜ë©° ì‹œê°„ì— ë”°ë¥¸ í‰ê·  ì´ë™ ì‹œê°„ì„ ê³„ì‚°í•˜ëŠ” ë° í™œìš©ë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ê·¸ë˜í”„ ë‚´ì˜ êµ¬ì¡°ì ì¸ ì†ì„±ê³¼ í´ëŸ¬ìŠ¤í„° ê°„ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

</aside>

the CTE will preserve the commute times distance in a Euclidean space. Note that this latent space of the nodes can not only be described spectrally but also in a parameter free-manner, which is not the case for other spectral embeddings, such as heat kernel or diffusion  maps as they rely on a time parameter t. More precisely, the embedding **matrix Z whose columns contain the nodesâ€™ commute times embeddings** is spectrally given by:

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/cf5decd5-b74d-4c1c-b12b-7ba666a21efe/Untitled.png)

where Î› is the diagonal matrix of the unnormalized **Laplacian L eigenvalues** and **F is the matrix of their associated eigenvectors**. Similarly, Î›0 contains the eigenvalues of the normalized Laplacian L and G the eigenvectors. We have F = GDâˆ’1/2 or fi = giDâˆ’1/2 , where D is the degree matrix.

## code snippet

```python
def graph_vol(G):
  A = nx.adjacency_matrix(G)
  D = A.sum(axis=1)
  D = D.squeeze()
  d = np.zeros(G.number_of_nodes())
  for i in range(G.number_of_nodes()):
    d[i] = D[0,i]
  vol = d.sum()
  return vol

def commute_times_embedding(G,e, evecs):
  n = G.number_of_nodes()
  vol = graph_vol(G)
  A = nx.adjacency_matrix(G)
  D = A.sum(axis=1)
  Phi = evecs
  Lambda = np.diag(e)
  Lambda = fractional_matrix_power(Lambda, -0.5)
  Lambda[0,0] = 0
  CTE = np.sqrt(vol)*np.matmul(Lambda,np.transpose(Phi))
  return CTE

CTE = commute_times_embedding(G,e,evecs)
CT = pdist(np.transpose(CTE), 'euclidean')
CT = squareform(CT)

# Commute Times as an optimzation problem 
NL = nx.normalized_laplacian_matrix(G)
NLdense = NL.todense()
# Finding eigen values and eigen vectors
eNL, evecsNL = np.linalg.eig(NLdense)
# Sort them (both e's and evecs's) ascending
idx =eNL.argsort()
eNL = eNL[idx]
evecsNL = evecsNL[:,idx]

# Let Z be the CT Embedding
# Initialize (first row with Zeros)
n = nx.number_of_nodes(G)
Z = np.zeros((1,3))
# Deterministic initialization
Z = np.concatenate((Z,np.ones((n-1,3))),axis = 0)
# Random initialization
Z = np.random.rand(n,3)
plt.imshow(Z, alpha=0.8, cmap="seismic")

def optimizing_commute_times_cost_function(Z, NLdense, lambdaReg):
  return np.trace(np.matmul(np.matmul(np.transpose(Z),NLdense),Z)) + \
 lambdaReg*np.linalg.norm(np.matmul(np.transpose(Z),Z)-np.eye(Z.shape[1]))
mu = 0.01
lambdaReg =0.1
cost_list=[]
embedding_list = []
embedding_list.append(Z)
maxiter = 1000
for i in range(maxiter):
  grad = 2*np.matmul(NLdense,Z) + lambdaReg*4*np.matmul(Z,np.matmul(np.transpose(Z),Z)-np.eye(Z.shape[1]))  #+ lambdaReg*4*np.matmul(np.transpose(Z),np.matmul(Z,np.transpose(Z))-np.eye(Z.shape[0]))
  Z = Z - mu*grad
  cost_list.append(optimizing_commute_times_cost_function(Z,NLdense,lambdaReg))
  embedding_list.append(Z)

CTZ = pdist(Z, 'euclidean')
CTZ = squareform(CTZ)

```

---

`Conclusion` **** 

- new idea for positional encoding
- 

---

## Reference

1. https://blog.twitter.com/engineering/en_us/topics/insights/2022/over-squashing--bottlenecks--and-graph-ricci-curvature
2. https://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15859-f11/www/notes/lecture11.pdf
3. Tortorella, Domenico, and Alessio Micheli. "Is Rewiring Actually Helpful in Graph Neural Networks?."Â *arXiv preprint arXiv:2305.19717*Â (2023)
4. Deac, Andreea, Marc Lackenby, and Petar VeliÄkoviÄ‡. "Expander graph propagation."Â *Learning on Graphs Conference*. PMLR, 2022.
5. Arnaiz-RodrÃ­guez, AdriÃ¡n, et al. "DiffWire: Inductive Graph Rewiring via the Lov\'asz Bound."Â *arXiv preprint arXiv:2206.07369*Â (2022).
6. Banerjee, Pradeep Kr, et al. "Oversquashing in GNNs through the lens of information contraction and graph expansion."Â *2022 58th Annual Allerton Conference on Communication, Control, and Computing (Allerton)*. IEEE, 2022.
7. Topping, Jake, et al. "Understanding over-squashing and bottlenecks on graphs via curvature."Â *arXiv preprint arXiv:2111.14522*Â (2021).
